<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Normalization and DE in RNA-seq data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kontantin Zaitsev" />
    <link rel="stylesheet" href="../libs/itmo.css" type="text/css" />
    <link rel="stylesheet" href="../libs/itmo-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Normalization and DE in RNA-seq data
### Kontantin Zaitsev
### November 9<sup>th</sup>, 2019

---


class: center, middle

# Normalizations, models

---

# Normalization

End goal for any kind of normalization is ability to perform a method (differential expression/PCA/clustering)

* We would like to make sure that variance in gene expression comes from biological variance + noise
* We would like to know how expression of a gene can be modeled: so we could perform a method
---

# RNA-seq studies

&lt;img src="data.png" width="80%"&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;Data and some images will be taken from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4878611/pdf/839.pdf&lt;/span&gt;&lt;/div&gt; 

---

# RNA-seq studies

&lt;img src="de_methods.png" width="60%"&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;Data and some images will be taken from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4878611/pdf/839.pdf&lt;/span&gt;&lt;/div&gt; 

---

# Good example dataset

To make sure we can make any comparisons we first will make sense of dataset that contains many replicates of the same biological sample

* 48 replicates of the same WT strain of Saccharomyces cerevisiae

---

## Libraries


```r
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("ggrepel", quietly = TRUE)) install.packages("ggrepel")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("DESeq2", quietly = TRUE)) BiocManager::install("DESeq2")
if (!requireNamespace("apeglm", quietly = TRUE)) BiocManager::install("apeglm")
if (!requireNamespace("pheatmap", quietly = TRUE)) install.packages("pheatmap")
if (!requireNamespace("vsn", quietly = TRUE)) BiocManager::install("vsn")
```

---
# Let's load the data


```r
library(DESeq2)
library(ggplot2)
library(vsn)
library(pheatmap)
library(ggrepel)
library(dplyr)
set.seed(1)

files &lt;- list.files("WT_countdata", full.names = T)
sampleNames &lt;- gsub(".*/(WT_rep\\d+_MID\\d+).*", "\\1", files)

counts &lt;- lapply(files, function(file){
  read.table(file, sep="\t", row.names = 1, header = 0)
})
```

---

# Let's load the data


```r
counts &lt;- do.call(cbind, counts)
colnames(counts) &lt;- sampleNames
counts[c(1:6, (nrow(counts) - 6):nrow(counts)), 1:3]
```

```
##                      WT_rep01_MID39 WT_rep02_MID50 WT_rep03_MID51
## 15S_rRNA                          2             12             31
## 21S_rRNA                         20             76            101
## HRA1                              3              2              2
## ICR1                             75            123            107
## LSR1                             60            163            233
## NME1                             13             14             23
## tY(GUA)O                          0              0              0
## tY(GUA)Q                          0              0              0
## no_feature                   409051         705266         571431
## ambiguous                    511292         644560         734023
## too_low_aQual                     0              0              0
## not_aligned                       0              0              0
## alignment_not_unique              0              0              0
```

---

# Removing unwanted lines


```r
counts &lt;- counts[1:(nrow(counts) - 5), ]
dim(counts)
```

```
## [1] 7126   48
```

---

## First things first:

* First thing that we would like to look at is library size
* In RNA-seq library size can vary significantly from sample to sample

---
# How coverage effects analysis


```r
ggplot(data=data.frame(librarySize=colSums(counts)), aes(x=librarySize)) +
  geom_histogram() + theme_bw() + scale_x_log10()
```

![](normalization_files/figure-html/unnamed-chunk-5-1.svg)&lt;!-- --&gt;

---
# How coverage effects analysis


```r
sampleByCoverage &lt;- colnames(counts)[order(colSums(counts))]
lowcov &lt;- sampleByCoverage[1]
highcov &lt;- sampleByCoverage[length(sampleByCoverage)]
```

---
# How coverage effects analysis


```r
ggplot(data=data.frame(highcov=counts[, highcov], 
                       lowcov=counts[, lowcov]), 
       aes(x=lowcov, y=highcov)) +
  geom_point() + theme_bw() + scale_x_log10() + scale_y_log10()
```

![](normalization_files/figure-html/unnamed-chunk-7-1.svg)&lt;!-- --&gt;

---

## Understanding counts and expression levels

Counts are amazing: 
* Within one sample: more counts a gene have, higher expression of a gene
* Counts represent number of reads aligned to a gene

Count are not amazing:
* We can not compare counts between two samples (due to library size)
* Actual "gene expression" is relative


---

# Going relative

Usually when we go to relative level we have to take into account at least two things:

* Library size
* Gene size


---

## Why gene size matters

.center[
&lt;img src="gene_size.png" width="80%"&gt;
]


---
## Does gene size always matter?

Library preparation, however, might be different:
.small[
* If we do full size sequencing (i.e. poly A selection -&gt; reverse transcription -&gt; fragmentation), then we have to account for gene size: number of reads is number of fragments per cDNA molecule * number of cDNA molecules
* If we sequence 3' for example: (i.e. fragmentation -&gt; poly A selection -&gt; reverse transcription), actual "gene size" is almost the same for all the genes =&gt; number of reads is a constant * number of cDNA molecules
* If we sequence with UMIs (unique molecule identifiers) constant above becomes 1
]

---

## Does gene size always matter?

.center[
&lt;img src="3prime.png" width="60%"&gt;
]

---
## Simple way: RPM - reads per million

Let's assume:
.small[
* `\(X\)` is a counts matrix, `\(x_{i, j}\)` is the number of reads of gene `\(i\)` in sample `\(j\)`
* `\(l_j = \sum_{i=1}^{genes}{x_{i, j}}\)` is a library size (sum of all reads per sample)
* Scaling factor is calculated simply: 
$$ f\_j = \frac{l\_j}{1 000 000} $$
* Scaled values are calculated easily:
$$ \tilde{x}\_{i, j} = \frac{x\_{i, j}}{f\_j} = x\_{i, j} \frac{1 000 000}{l\_j}$$
]

---

## RPKM/FPKM

.small[
* Assume we have full-length sequencing
* `\(g_i\)` is a the size of a gene `\(i\)` in kilobases (K)
* Reads Per Kilobase Million (RPKM) are RPM devided by gene size:
$$ \tilde{x}\_{i, j} = \frac{x\_{i, j}}{f\_j g\_i} = \frac{x\_{i, j}}{g\_i} \frac{1 000 000}{l\_j}$$
* FPKM (fragments per kilobase million) is the same thing but for paired-end sequensing we will count fragments instead of reads
]


&lt;div class="my-footer"&gt;&lt;span&gt;Best link to read about it is https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/&lt;/span&gt;&lt;/div&gt; 

---

## TPM (Transcripts Per Kilobase Million)

.small[
TPM is pretty much the same, but the order is different: we first normalize for gene length and then calculate scaling factor
* We first calculate RPK (reads per kilobase)
$\tilde{{x}\_{i, j}} = \frac{x\_{i, j}}{g\_i} $
* Library size is calculated for adjusted values:
`\(\tilde{l_j} = \sum_{i=1}^{genes}{\tilde{x_{i, j}}}\)`
* TPM are RPK devided by per million scaling factor:
$$ \bar{x}\_{i, j} = \tilde{x}\_{i, j} \frac{1 000 000}{\tilde{l\_j}} = \frac{x\_{i, j}}{g\_i} \frac{1 000 000}{\tilde{l\_j}} $$
]


&lt;div class="my-footer"&gt;&lt;span&gt;Best link to read about it is https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/&lt;/span&gt;&lt;/div&gt; 

---

## RPM/RPKM/FPKM/TPM????

* RPM/TPM guarantees that after normalization sum of normalized counts will be the same (1 000 000)
* TPM also accounts for gene size
* General advice: we use TPM and we suggest you also use TPM if you need normalized data in linear scale

---

## Lets compare RPM counts


```r
ggplot(data=data.frame(highcov= 1000000 * counts[, highcov] / sum(counts[, highcov]), 
                       lowcov=  1000000 * counts[, lowcov] / sum(counts[, lowcov])), 
       aes(x=lowcov, y=highcov)) +
  geom_point() + theme_bw() + scale_x_log10() + scale_y_log10()
```

![](normalization_files/figure-html/unnamed-chunk-8-1.svg)&lt;!-- --&gt;

---

## Lowly expressed genes are noisy

Relative expression is something very similar to probability:

* If gene A has an RPM of 10 000 (out of 1 000 000), it means that if you took a random read from this sample, probability of this read being aligned to gene A is 0.01
* When it comes to lowly expressed genes these probabilities are very low
* When it comes to lowly expressed genes signal becomes noisy

---

## Before we start let's create DESeq2 object


```r
dds &lt;- DESeqDataSetFromMatrix(counts, 
                              data.frame(strain=rep("WT", ncol(counts)),
                                         libsize=colSums(counts),
                                         sampleName=sampleNames,
                                         row.names = sampleNames), design=~1)
dds &lt;- dds[rowSums(counts(dds)) &gt; 0, ]
dds &lt;- DESeq(dds)
```

---

## Before we start let's create DESeq2 object


```r
hist(log10(colSums(counts(dds))), main = "Library size")
hist(log10(colSums(counts(dds, normalized=T))), main = "Adjusted library size")
```

![](normalization_files/figure-html/unnamed-chunk-10-1.svg)![](normalization_files/figure-html/unnamed-chunk-10-2.svg)

---

## Methods and normalizations:

Usual log-transformation: log2(x + 1):
* Keeps values positive
* Easy to interpret
* A lot of noise in lowly abundant trasncripts
* Don't use for DE or PCA (because of the variance issues)

It is ok to use for heatmaps, if you only want to highlight serveral genes that are DE

---

## Usual log2 transform


```r
ntd &lt;- normTransform(dds)
meanSdPlot(assay(ntd), ranks = F)
meanSdPlot(assay(ntd)) + theme_bw()
```

```
## NULL
```

![](normalization_files/figure-html/unnamed-chunk-11-1.svg)![](normalization_files/figure-html/unnamed-chunk-11-2.svg)

---
## Usual log2 transform

* Lowly expressed genes explain to much variance for PCA
* Lowly expressed genes might be hard to interpret on a heatmap
* Sometimes we filter lowly expressed genes (as we did with microarray), however, choosing a threshold is also a complicated task

---

## Regularized log transformation

* Regularized log2 transformation shrinks dispersions towards gene expression mean
* Let's denote `\(q_{i, j}\)` is a parameter proportional to the expected true concentration of fragments for gene `\(i\)` and sample `\(j\)`
* We will fit generalized linear model (yeah, again):
$$ log2(q_{i, j}) = \beta_{i, 0} + \beta_{i, j} $$
* `\(\beta_{i, 0}\)` is a gene mean expression and will won't be shrunk, `\(\beta_{i,j}\)` is the sample-specific effect which is shrunk toward zero based on the dispersion-mean trend over the entire dataset

---
## Regularized log transformation

* Easy to interpret
* Might contain negative value for lowly abundant transcripts
* Variance is **shrunk** for lowly abundant transcripts
* Lowly abundunt transcript will not introduce variance
* OK for: PCA, Heatmaps
* Great: when you try to cluster samples hierarchically on a heatmap

---

## Regularized log transformation


```r
rld &lt;- rlogTransformation(dds)
meanSdPlot(assay(rld), ranks = F)
meanSdPlot(assay(rld)) + theme_bw()
```

```
## NULL
```

![](normalization_files/figure-html/unnamed-chunk-12-1.svg)![](normalization_files/figure-html/unnamed-chunk-12-2.svg)

---

## Variance stabilizing transformation 

* Some methods (like PCA) require that your data is kinda "in the same scale"
* PCA calculates variance that comes from changes in gene expression, and find principal component based on that variance
* If lowly expressed genes introduced too much variance into PCA then our analysis is biased
* If lowly expressed genes introduced too little variance into PCA then our analysis is biased
* Variance stabilizing transformation is a solution
* Almost the same mechanism as rlog but dispersion is shrunk using global experiment-wide trend (but not gene-wise levels)

---

## Variance stabilizing transformation 

* A bit harder to interpret (minimal value is 3.52 for example, what does that mean ??)
* Variance is **shrunk** for all the transcripts to the same level
* All transcripts introduce variance
* Great for PCA

---

## Variance stabilizing transformation 


```r
vsd &lt;- varianceStabilizingTransformation(dds)
meanSdPlot(assay(vsd), ranks = F) 
meanSdPlot(assay(vsd)) + theme_bw()
```

```
## NULL
```

![](normalization_files/figure-html/unnamed-chunk-13-1.svg)![](normalization_files/figure-html/unnamed-chunk-13-2.svg)

---
## Comparing PCAs


```r
plotPCA(ntd, "libsize") + theme_bw()
plotPCA(rld, "libsize") + theme_bw()
plotPCA(vsd, "libsize") + theme_bw()
```

![](normalization_files/figure-html/unnamed-chunk-14-1.svg)![](normalization_files/figure-html/unnamed-chunk-14-2.svg)![](normalization_files/figure-html/unnamed-chunk-14-3.svg)

---
## Comparing PCAs


```r
plotPCA(vsd, "libsize") + geom_text_repel(aes(label=name), size=2) + theme_bw()
```

![](normalization_files/figure-html/unnamed-chunk-15-1.svg)&lt;!-- --&gt;

---

## Once we done with one condition

* We can start making comparisons
* But before we do that, lets also look if KO samples have outliers

---
## KO samples


```r
filesSnf2 &lt;- list.files("Snf2_countdata", full.names = T)
sampleNamesSnf2 &lt;- gsub(".*/(Snf2_rep\\d+_MID\\d+).*", "\\1", filesSnf2)

countsSnf2 &lt;- lapply(filesSnf2, function(file){
  read.table(file, sep="\t", row.names = 1, header = 0)
})

countsSnf2 &lt;- do.call(cbind, countsSnf2)
colnames(countsSnf2) &lt;- sampleNamesSnf2
countsSnf2 &lt;- countsSnf2[1:(nrow(countsSnf2) - 5), ]
dim(countsSnf2)
```

```
## [1] 7126   48
```
---

## Before we start let's create DESeq2 object


```r
ddsSnf2 &lt;- DESeqDataSetFromMatrix(countsSnf2, 
                              data.frame(strain=rep("snf2_ko", ncol(countsSnf2)),
                                         libsize=colSums(countsSnf2),
                                         sampleName=sampleNamesSnf2,
                                         row.names = sampleNamesSnf2), design=~1)
ddsSnf2 &lt;- ddsSnf2[rowSums(counts(ddsSnf2)) &gt; 0, ]
ddsSnf2 &lt;- DESeq(ddsSnf2)
ntdSnf2 &lt;- normTransform(ddsSnf2)
rldSnf2 &lt;- rlogTransformation(ddsSnf2)
vsdSnf2 &lt;- varianceStabilizingTransformation(ddsSnf2)
```
---
## Comparing PCAs


```r
plotPCA(ntdSnf2, "libsize") + theme_bw()
plotPCA(rldSnf2, "libsize") + theme_bw()
plotPCA(vsdSnf2, "libsize") + theme_bw()
```

![](normalization_files/figure-html/unnamed-chunk-18-1.svg)![](normalization_files/figure-html/unnamed-chunk-18-2.svg)![](normalization_files/figure-html/unnamed-chunk-18-3.svg)

---
## Comparing PCAs


```r
plotPCA(vsdSnf2, "libsize") + geom_text_repel(aes(label=name), size=2) + theme_bw()
```

![](normalization_files/figure-html/unnamed-chunk-19-1.svg)&lt;!-- --&gt;

---

## Lets combine the datasets together without outliers


```r
counts &lt;- counts[, -c(21, 22, 25, 28)]
counts &lt;- counts[, sample(ncol(counts), 5)]
countsSnf2 &lt;- countsSnf2[, -c(6, 13, 35)]
countsSnf2 &lt;- countsSnf2[, sample(ncol(countsSnf2), 5)]

ddsMerged &lt;- DESeqDataSetFromMatrix(
  cbind(counts, countsSnf2), 
  data.frame(strain=c(rep("wt", ncol(counts)), rep("snf2_ko", ncol(countsSnf2))),
            libsize=c(colSums(counts), colSums(countsSnf2)),
            sampleName=c(colnames(counts), colnames(countsSnf2))), 
  design=~strain)
ddsMerged &lt;- ddsMerged[rowSums(counts(ddsMerged)) &gt; 0, ]
ddsMerged &lt;- DESeq(ddsMerged)
ntdMerged &lt;- normTransform(ddsMerged)
rldMerged &lt;- rlogTransformation(ddsMerged)
vsdMerged &lt;- varianceStabilizingTransformation(ddsMerged)
```

---

## Lets combine the datasets together without outliers


```r
plotPCA(ntdMerged, "strain") + theme_bw()
plotPCA(vsdMerged, "strain") + theme_bw()
```

![](normalization_files/figure-html/unnamed-chunk-21-1.svg)![](normalization_files/figure-html/unnamed-chunk-21-2.svg)

---
## Probabilites and NB-distribution

For differential expression we don't need to normalize expression values: we can try to understand the probabilities from count data

* Negitive binomial (NB) distribution
* NB(r, p): "is a discrete probability distribution of the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures (denoted r) occurs"

&lt;div class="my-footer"&gt;&lt;span&gt;Wiki,  https://en.wikipedia.org/wiki/Negative_binomial_distribution&lt;/span&gt;&lt;/div&gt; 

---

## NB(r, p)

* Let's assume I take one read at a time from all the reads that come from sample `\(j\)`
* If a read I take is coming from gene `\(i\)`, it's a success
* If not, it's a failure
* p - probability of read to be from a gene `\(j\)` (probability of success)
* p - is something very close to true expression

&lt;div class="my-footer"&gt;&lt;span&gt;Wiki,  https://en.wikipedia.org/wiki/Negative_binomial_distribution&lt;/span&gt;&lt;/div&gt; 

---

## Lets play a bit in R

* Careful, in R, `\(NB(r, p)\)` returns number of **failures** and `\(r\)` is number of successes
* Simple dice example: we roll the dice, if the number of the dice is 5 or 6, we win
* How many times wil we lose, before we win 5 times?

---
## Lets play a bit in R

* Careful, in R, `\(NB(r, p)\)` returns number of **failures** and `\(r\)` is number of successes

```r
plot(dnbinom(1:40, size=5, prob=1/3))
```

![](normalization_files/figure-html/unnamed-chunk-22-1.svg)&lt;!-- --&gt;

---
## Lets play a bit in R

* However, in R, `\(NB(r, 1 - p)\)` will return number of **successes** if `\(r\)` is number of failures

```r
plot(dnbinom(1:40, size=10, prob=1 - 1/3))
```

![](normalization_files/figure-html/unnamed-chunk-23-1.svg)&lt;!-- --&gt;

---
## Gene expression question

* Given that gene is lowly expressed gene i.e. `\(p=10^{-6}\)`, and we sequence reads one at a time
* How many reads from this gene I will I sequence before I sequence 10 000 000 reads that are not from this gene?

---

## Gene expression question: 10 000 000 reads


```r
hist(rnbinom(100, size=1e+7, prob=1 - 1e-6))
```

![](normalization_files/figure-html/unnamed-chunk-24-1.svg)&lt;!-- --&gt;

---

## Gene expression question: 1 000 000 reads


```r
hist(rnbinom(100, size=1e+6, prob=1 - 1e-6))
```

![](normalization_files/figure-html/unnamed-chunk-25-1.svg)&lt;!-- --&gt;

---

## Gene expression question: 100 000 reads


```r
hist(rnbinom(100, size=1e+5, prob=1 - 1e-6))
```

![](normalization_files/figure-html/unnamed-chunk-26-1.svg)&lt;!-- --&gt;

---
## NB(r, p)

For `\(NB(r, p)\)` we know both the mean and variance based on `\(r\)` and `\(p\)`. For `\(X \sim NB(r, p)\)`

* Mean: `\(E[X] = \frac{pr}{1 - p}\)`
* Variance: `\(Var(x) = \frac{pr}{(1 - p)^2}\)`

This is extremely useful, because we can model our distrubiton using mean and variance (which are easier to calculate) and then get `\(r\)` and `\(p\)` if needed

---

## Spoiler: next slide is complicated even for me

.center[
&lt;img src="meme.jpg" width="50%"&gt;
]

---

## Differential expression

.small[
Now that we know how NB works, lets look at how DESeq2 calculates DE (for 2 conditions A vs B):
$$ K\_{i, j} \sim NB(\mu\_{i, j}, \alpha\_i) $$
$$ \mu\_{i, j} = s\_{j}\  p\_{i, j} $$
$$ log2(p\_{i, j}) = x\_{j, A} \beta \ \_{i, A} +  x\_{j, B} \  \beta\_{i, B}$$
]

.tiny[
* Where, `\(K_{i, j}\)` is matrix of observed counts (known), 
* `\(\mu_{i, j}\)` is a mean for NB distribuion,
* `\(p_{i, j}\)` is a probability to get read `\(i\)` from sample `\(j\)`
* `\(s_j\)` is a scaling factor (will be calculated), `\(\alpha_i\)` are gene dispersions (will be calculated),
* matrix `\(x\)` is model coefficients (zero or one depending on conditions) and most importantly
* `\(\beta_{i, j}\)` probability to get read from gene `\(i\)` if a sample is from condition
]

---

## Differential expression

.small[
Now that we know how NB works, lets look at how DESeq2 calculates DE (for 2 conditions A vs B):
$$ K\_{i, j} \sim NB(\mu\_{i, j}, \alpha\_i) $$
$$ \mu\_{i, j} = s\_{j}\  p\_{i, j} $$
$$ log2(p\_{i, j}) = x\_{j, A} \beta \ \_{i, A} +  x\_{j, B} \  \beta\_{i, B}$$
]

DESeq2 works as follows
.tiny[
1. estimation of size factors `\(s_j\)` by estimateSizeFactors
2. estimation of dispersion `\(\alpha_i\)` by estimateDispersions
3. negative binomial GLM fitting for `\(\beta_A\)` and `\(\beta_B\)` and Wald statistics by nbinomWaldTest
]

---

## Differential expression

Wald test will compare contrast (difference) of these two `\(\beta_A\)` and `\(\beta_B\)`:

$$ \sqrt(W) = \frac{(\beta\_A - \beta\_B)}{se(\beta\_A - \beta\_B)} $$
* `\(\sqrt(W)\)` will follow standard normal distribution and we can calculate p value using this statistic
* P value are corrected using Benjamini Hochberg (BH) correction

---

## Running DE


```r
res &lt;- results(ddsMerged)
res &lt;- as.data.frame(res)
res$gene &lt;- rownames(res)
ggplot(res, aes(x=log2FoldChange, y=-log10(padj), color=padj &lt; 0.05)) +
  geom_point() + theme_bw() + scale_color_manual(values=c("black", "red"))
  # geom_text_repel(data=res %&gt;% dplyr::filter(padj &lt; 1e-40), aes(label=gene, color=NULL))
```

![](normalization_files/figure-html/unnamed-chunk-27-1.svg)&lt;!-- --&gt;


---

## Running DE: LFC shrink


```r
res &lt;- lfcShrink(ddsMerged, coef="strain_wt_vs_snf2_ko", type="apeglm")
res &lt;- as.data.frame(res)
res$gene &lt;- rownames(res)
ggplot(res, aes(x=log2FoldChange, y=-log10(padj), color=padj &lt; 0.05)) +
  geom_point() + theme_bw() + scale_color_manual(values=c("black", "red"))
  # geom_text_repel(data=res %&gt;% dplyr::filter(padj &lt; 1e-40), aes(label=gene, color=NULL))
```

![](normalization_files/figure-html/unnamed-chunk-28-1.svg)&lt;!-- --&gt;

---

## Questions?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
